{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pto3Sog0hnK9"
      },
      "source": [
        "### tsdGAN: A generative adversarial network approach for removing electrocardiographic interference from electromyographic signals \n",
        "Lucas Haberkamp<sup>1,2</sup>, Charles A. Weisenbach<sup>1</sup>, Peter Le<sup>3</sup>  \n",
        "<sup>1</sup>Naval Medical Research Unit Dayton, Wright-Patterson Air Force Base, OH, USA   \n",
        "<sup>2</sup>Leidos, Reston, VA, USA   \n",
        "<sup>3</sup>Air Force Research Laboratory, 711th Human Performance Wing, Wright-Patterson Air Force Base, OH, USA\n",
        "\n",
        "#### This notebook is used to train the tsdGAN deep learning model on experimental data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_eoq19VDh-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn import utils\n",
        "from tensorflow.keras.activations import gelu\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Butterworth filter function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def butterfilter(x, Fc, Fs, ftype):\n",
        "    Wn = np.asarray(Fc)/np.asarray(Fs/2)\n",
        "    b, a = signal.butter(2, Wn, ftype)\n",
        "    return signal.filtfilt(b,a,x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load in training & validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vlyNp4jhWjY"
      },
      "outputs": [],
      "source": [
        "# Experimental dataset\n",
        "x_train = np.load('../../Data/Preprocessed Data/Experimental/x_real.npy')\n",
        "y_train = np.load('../../Data/Preprocessed Data/Experimental/y_real.npy')\n",
        "label_train = np.load('../../Data/Preprocessed Data/Experimental/label_real.npy')\n",
        "\n",
        "# Reshape experimental data so that it has only 1 feature\n",
        "x_train = np.reshape(x_train.transpose(2,0,1),(-1, x_train.shape[1],1))\n",
        "y_train = np.reshape(y_train.transpose(2,0,1),(-1, y_train.shape[1],1))\n",
        "label_train = np.reshape(label_train.transpose(2,0,1),(-1, label_train.shape[1],1))\n",
        "\n",
        "# Load an experimental trial used for validation \n",
        "val_df = pd.read_csv('../../Data/Raw TS EMG Data/Validation/TrunkStability_DS_S20_EMG_Raw_17.csv', header=13).iloc[:,3:]\n",
        "val_df = val_df - val_df.mean()\n",
        "val_df = val_df.apply(lambda x: butterfilter(x, Fc=500, Fs=1920, ftype='low'))\n",
        "val_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mask the ground truth data so that it does not contain QRS locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZOPXEVjqov",
        "outputId": "85a2e97a-859a-4b91-d602-baab59738ab0"
      },
      "outputs": [],
      "source": [
        "y_target = []\n",
        "for i in range(x_train.shape[0]):\n",
        "    y_target.append(x_train[i] * label_train[i])\n",
        "y_target = np.array(y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"x_shape:\", x_train.shape)\n",
        "print(\"y_shape:\", y_train.shape)\n",
        "print(\"y_target:\", y_target.shape)\n",
        "print(\"label_shape:\", label_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d7SiVyYv0FAa",
        "outputId": "9d38329d-fcf0-485d-f1c8-234e6140f152"
      },
      "outputs": [],
      "source": [
        "# Confirm the synthetic data is unpaired\n",
        "for i in range(5):\n",
        "  plt.plot(x_train[i])\n",
        "  plt.plot(y_train[i])\n",
        "  plt.title(\"Unpaired Synthetic Data: \" + str(i))\n",
        "  plt.xlabel(\"Samples\")\n",
        "  plt.ylabel(\"Normalized Amplitude\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D80sGabItgkH",
        "outputId": "2756c871-920f-4e7f-ae3d-8c647f60a1ea"
      },
      "outputs": [],
      "source": [
        "# Confirm the QRS complexes are masked \n",
        "for i in range(5):\n",
        "    plt.plot(y_target[i])\n",
        "    plt.title(\"Masked Target Data: \" + str(i))\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Normalized Amplitude\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcYiM5A5GDaN",
        "outputId": "b8915938-3223-42a3-972a-72073a2eed75"
      },
      "outputs": [],
      "source": [
        "# Plot QRS complex locations over the contaminated data\n",
        "for i in range(10):\n",
        "    plt.plot(x_train[i])\n",
        "    plt.plot(label_train[i])\n",
        "    plt.title('QRS Complex Label: ' + str(i))\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Normalized Amplitude\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the TCN separation module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U8LT2D21e6x"
      },
      "outputs": [],
      "source": [
        "def tcn_block(inputs, filters, dilation_rates):\n",
        "  res = LayerNormalization()(inputs)\n",
        "  for dilation in dilation_rates:\n",
        "    x = SeparableConv1D(filters, dilation_rate=dilation,\n",
        "                        kernel_size=3, padding='same', activation=gelu)(res)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = SeparableConv1D(filters, dilation_rate=dilation,\n",
        "                        kernel_size=3, padding='same', activation=gelu)(x)\n",
        "    res = LayerNormalization()(Add()([x, res]))\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWcDiO59KJr9"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  dilation_rates = [1,2,4,8,16,32,64]\n",
        "\n",
        "  inputs = Input(shape=(None, 1))\n",
        "\n",
        "  x = Conv1D(filters=32, kernel_size=7, padding='same', activation=gelu)(inputs)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=128, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=128, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "\n",
        "  sep = tcn_block(x, filters=128, dilation_rates=dilation_rates)\n",
        "  sep = Conv1D(filters=128, kernel_size=1, padding='same', activation='sigmoid')(sep)\n",
        "\n",
        "  mask = Multiply()([x, sep])\n",
        "  mask = LayerNormalization()(mask)\n",
        "\n",
        "  x = Conv1DTranspose(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(mask)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1DTranspose(filters=32, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  outputs = Conv1D(1, kernel_size=7, padding='same')(x)\n",
        "  return Model(inputs, outputs, name='generator')\n",
        "\n",
        "model = build_generator()\n",
        "model.summary()\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itoO4OitvuDn"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "  dilation_rates = [1,2,4,8,16,32,64,128]\n",
        "\n",
        "  inputs = Input(shape=(None, 1))\n",
        "\n",
        "  noise = GaussianNoise(0.1)(inputs)\n",
        "\n",
        "  x = Conv1D(filters=32, kernel_size=7, padding='same', activation=gelu)(noise)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "\n",
        "  x = tcn_block(x, filters=64, dilation_rates=dilation_rates)\n",
        "\n",
        "  x = Conv1D(filters=64, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "  x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "  outputs = Dense(1)(x)\n",
        "\n",
        "  return Model(inputs, outputs, name='discriminator')\n",
        "\n",
        "model = build_discriminator()\n",
        "model.summary()\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to create & compile the generator & discriminator neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzxTAtsZOYZJ"
      },
      "outputs": [],
      "source": [
        "def create_gans():\n",
        "\n",
        "    # create building blocks\n",
        "    discriminator = build_discriminator()\n",
        "    generator = build_generator()\n",
        "\n",
        "    # compile discriminators while they're set to trainable\n",
        "    optimizer = Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "    discriminator.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # create the first GAN architecture\n",
        "    input_seq = Input(shape=(None,1))\n",
        "    input_weights = Input(shape=(None,1))\n",
        "\n",
        "    output_seq_b = generator(input_seq)\n",
        "    global_out = discriminator(output_seq_b)\n",
        "    mask_output_seq_b = Multiply(name='reconstruction')([output_seq_b, input_weights])\n",
        "\n",
        "    GAN = Model([input_seq, input_weights], [global_out, mask_output_seq_b], name='GAN')\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-4, beta_1=0.5)\n",
        "    loss = ['mse', 'mae']\n",
        "    loss_weights = [1, 10]\n",
        "\n",
        "    GAN.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights)\n",
        "\n",
        "    return GAN, generator, discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ370E0jG9i0"
      },
      "outputs": [],
      "source": [
        "GAN, generator, discriminator = create_gans()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to evaluate tsdGAN performance on experimental data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI-sPmSXBOGt"
      },
      "outputs": [],
      "source": [
        "# Initialize a scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def tsdGAN_filter(raw_data, window_size=2880):\n",
        "    data = raw_data.values\n",
        "\n",
        "    step_size = window_size // 3\n",
        "    x_test = []\n",
        "\n",
        "    # Generate windows\n",
        "    for i in range(window_size, data.shape[0], step_size):\n",
        "        x_test.append(data[i-window_size:i])\n",
        "    \n",
        "    # Handle the last window if there's remaining data\n",
        "    last_data_start = i\n",
        "    if last_data_start < data.shape[0]:\n",
        "        x_test.append(data[-window_size:])\n",
        "    \n",
        "    x_test = np.array(x_test)\n",
        "    \n",
        "    # Scale the data\n",
        "    x_test_scaled = []\n",
        "    for window in x_test:\n",
        "        x_test_scaled.append(scaler.fit_transform(window.reshape(-1, 1)))\n",
        "    x_test_scaled = np.array(x_test_scaled)\n",
        "\n",
        "    # Predict on scaled data\n",
        "    y_pred = model.predict(x_test_scaled, batch_size=32)\n",
        "\n",
        "    # Inverse transform the predictions\n",
        "    y_pred_inverse = []\n",
        "    for i in range(y_pred.shape[0]):\n",
        "        scaler.fit(x_test[i].reshape(-1, 1))\n",
        "        y_pred_inverse.append(scaler.inverse_transform(y_pred[i]))\n",
        "    y_pred_inverse = np.array(y_pred_inverse)\n",
        "\n",
        "    # Prepare the final array\n",
        "    final_output = [y_pred_inverse[0][:1920]]  # Start with the first segment's initial part\n",
        "\n",
        "    # Add middle segments, adjust if necessary to include more of each segment\n",
        "    for seg in y_pred_inverse[1:-1]:\n",
        "        final_output.append(seg[step_size:-step_size])\n",
        "\n",
        "    # Handle the last segment to match the remaining data length\n",
        "    final_len = data.shape[0] - last_data_start + step_size\n",
        "    final_segment = y_pred_inverse[-1]\n",
        "    final_output.append(final_segment[-final_len:])\n",
        "\n",
        "    # Concatenate the adjusted segments\n",
        "    final_output = np.concatenate(final_output, axis=0)\n",
        "\n",
        "    return final_output.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYqU6XDiWGek"
      },
      "outputs": [],
      "source": [
        "# Reset states generated by Keras\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_DVNmix4G9i0",
        "outputId": "37369cb7-f791-4fa7-ddca-947e267a45c1"
      },
      "outputs": [],
      "source": [
        "# Core training loop, loads batch of sequences, generators and discriminators on batch\n",
        "batch_size = 64\n",
        "\n",
        "# Calculate number of batches\n",
        "num_batches = x_train.shape[0] // batch_size\n",
        "\n",
        "# total epochs \n",
        "epochs = 150\n",
        "\n",
        "epoch_count = 1 # index of current epoch\n",
        "\n",
        "# Empty arrays for storing loss\n",
        "real_loss_all = []\n",
        "fake_loss_all = []\n",
        "gan_loss_all = []\n",
        "\n",
        "while epoch_count <= epochs:\n",
        "    start_time = time.time() # timer for the epoch to complete\n",
        "\n",
        "    # shuffle training data\n",
        "    x_train, y_train, y_target, label_train = utils.shuffle(x_train, y_train, y_target, label_train)\n",
        "\n",
        "    print('EPOCH: ' + str(epoch_count))\n",
        "\n",
        "    # Get a batch of data\n",
        "    batch_index = 0  # index of the current batch\n",
        "\n",
        "    # Store per batch loss within the epoch\n",
        "    real_loss = []\n",
        "    fake_loss = []\n",
        "    gan_loss = []\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Select a subset of the data corresponding to the indices of the batch\n",
        "        seq_a_batch = x_train[batch_index:batch_index+batch_size]\n",
        "        seq_b_batch = y_train[batch_index:batch_index+batch_size]\n",
        "        masked_seq_a_batch = y_target[batch_index:batch_index+batch_size]\n",
        "        labels_batch = label_train[batch_index:batch_index+batch_size]\n",
        "\n",
        "        batch_index += batch_size\n",
        "\n",
        "        target_batch = np.ones([len(seq_a_batch),1])\n",
        "        fake_batch = np.zeros([len(seq_a_batch),1]) \n",
        "\n",
        "        # Train discriminator on real data\n",
        "        loss = discriminator.train_on_batch(seq_b_batch, target_batch)\n",
        "        real_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "\n",
        "        # Use the generator to create predictions for the discrminator\n",
        "        seq_b_batch_fake = generator.predict(seq_a_batch, verbose=0)\n",
        "\n",
        "        # Train discriminator on fake data\n",
        "        loss = discriminator.train_on_batch(seq_b_batch_fake, fake_batch)\n",
        "        fake_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "        \n",
        "        # Fit each generator\n",
        "        loss = GAN.train_on_batch([seq_a_batch, labels_batch], [target_batch, masked_seq_a_batch])\n",
        "        gan_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "\n",
        "    gan_loss = np.mean(np.concatenate(gan_loss, axis=0), axis=0)\n",
        "    real_loss = np.mean(np.concatenate(real_loss, axis=0), axis=0)\n",
        "    fake_loss = np.mean(np.concatenate(fake_loss, axis=0), axis=0)\n",
        "\n",
        "    print(\"--- %s seconds ---\" % np.round((time.time() - start_time),2))\n",
        "    print(\"Discriminator Real Data:\", discriminator.metrics_names[0], \"=\", real_loss)\n",
        "    print(\"Discriminator Fake Data:\", discriminator.metrics_names[0], \"=\", fake_loss)\n",
        "    print(\"GAN:\", GAN.metrics_names[0], \"=\", gan_loss[0], \"-\", GAN.metrics_names[1], \"=\", gan_loss[1], \"-\", GAN.metrics_names[2], \"=\", gan_loss[2])\n",
        "    \n",
        "    gan_loss_all.append(np.expand_dims(gan_loss, axis=0))\n",
        "    fake_loss_all.append(np.expand_dims(fake_loss, axis=0))\n",
        "    real_loss_all.append(np.expand_dims(real_loss, axis=0))\n",
        "\n",
        "    # Validate performance every 10 epochs\n",
        "    if (epoch_count % 10 == 0):\n",
        "\n",
        "        # Save models\n",
        "        MODEL_PATH = '../../Models/Experimental/discriminator_epoch' + str(epoch_count) + '.h5'\n",
        "        discriminator.save(MODEL_PATH)\n",
        "\n",
        "        MODEL_PATH = '../../Models/Experimental/generator_epoch' + str(epoch_count) + '.h5'\n",
        "        generator.save(MODEL_PATH)\n",
        "\n",
        "        # Save current training performance\n",
        "        np.save('../../Training Performance/Experimental/real_loss.npy', np.concatenate(real_loss_all, axis=0))\n",
        "        np.save('../../Training Performance/Experimental/fake_loss.npy', np.concatenate(fake_loss_all, axis=0))\n",
        "        np.save('../../Training Performance/Experimental/gan_loss.npy', np.concatenate(gan_loss_all, axis=0))\n",
        "\n",
        "        print(\"\\nEpoch \" + str(epoch_count) + \" Prediction\")\n",
        "\n",
        "        raw_df = val_df.copy()\n",
        "        filt_df = raw_df.apply(lambda x: tsdGAN_filter(x))\n",
        "\n",
        "        # Define a new set of more descriptive names for each EMG sensor\n",
        "        better_muscle_names = ['Right Erector Spinae',\n",
        "            'Left Erector Spinae',\n",
        "            'Right Internal Oblique',\n",
        "            'Left Internal Oblique',\n",
        "            'Right Latissimus Dorsi',\n",
        "            'Left Latissimus Dorsi',\n",
        "            'Right Rectus Abdominis',\n",
        "            'Left Rectus Abdominis',\n",
        "            'Right External Oblique',\n",
        "            'Left External Oblique']\n",
        "\n",
        "        # Define time-axis\n",
        "        t = np.arange(len(filt_df))/1920\n",
        "\n",
        "        # Creating a 5x2 grid of subplots\n",
        "        fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(12, 12))\n",
        "        fig.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust space between plots\n",
        "\n",
        "        c = 0  # Counter for iterating through DataFrame columns\n",
        "        for i in range(5):\n",
        "            for j in range(2):\n",
        "                ax = axs[i, j]\n",
        "                # Plotting raw and filtered data\n",
        "                ax.plot(t, raw_df.iloc[:, c], label=\"Raw\", color=\"black\", linewidth=1.5)\n",
        "                ax.plot(t, filt_df.iloc[:, c], label=\"tsdGAN\", color=\"dodgerblue\", linewidth=1.25)\n",
        "\n",
        "                # Setting titles, labels, and grid only for specific subplots\n",
        "                if i == 4:\n",
        "                    ax.set_xlabel(\"Time (s)\")\n",
        "                if j == 0:\n",
        "                    ax.set_ylabel(\"Amplitude (mV)\")\n",
        "\n",
        "                ax.set_title(better_muscle_names[c])\n",
        "\n",
        "                if (i, j) == (0, 0):  # Showing legend only in the first subplot\n",
        "                    ax.legend()\n",
        "\n",
        "                c += 1\n",
        "\n",
        "        plt.tight_layout()  # Adjust overall layout\n",
        "        plt.show()\n",
        "\n",
        "    epoch_count += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed17bba15b2f81cf1230037794da0c0f111480f5f2f8146275a9e25508496a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
