{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pto3Sog0hnK9"
      },
      "source": [
        "### tsdGAN: A generative adversarial network approach for removing electrocardiographic interference from electromyographic signals \n",
        "Lucas Haberkamp<sup>1,2</sup>, Charles A. Weisenbach<sup>1</sup>, Peter Le<sup>3</sup>  \n",
        "<sup>1</sup>Naval Medical Research Unit Dayton, Wright-Patterson Air Force Base, OH, USA   \n",
        "<sup>2</sup>Leidos, Reston, VA, USA   \n",
        "<sup>3</sup>Air Force Research Laboratory, 711th Human Performance Wing, Wright-Patterson Air Force Base, OH, USA\n",
        "\n",
        "#### This notebook is used to train the tsdGAN deep learning model on synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_eoq19VDh-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn import utils\n",
        "from tensorflow.keras.activations import gelu\n",
        "import pandas as pd\n",
        "from scipy import signal, spatial\n",
        "import time\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load in training & validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vlyNp4jhWjY"
      },
      "outputs": [],
      "source": [
        "# Synthetic dataset\n",
        "x_train = np.load('../../Data/Preprocessed Data/Synthetic/Training/x_syn.npy')\n",
        "y_train = np.load('../../Data/Preprocessed Data/Synthetic/Training/y_syn.npy')\n",
        "label_train = np.load('../../Data/Preprocessed Data/Synthetic/Training/label_syn.npy')\n",
        "\n",
        "# Function to convert validation dictionary to array\n",
        "def dict2arr(dictionary):\n",
        "  x = []\n",
        "  for i, value in enumerate(dictionary.values()):\n",
        "    x.append(value)\n",
        "  return np.vstack(x)\n",
        "\n",
        "with open('../../Data/Preprocessed Data/Synthetic/Validation/x_val.pkl', 'rb') as f:\n",
        "  x_val = pickle.load(f)\n",
        "\n",
        "x_val = dict2arr(x_val)\n",
        "print(x_val.shape)\n",
        "\n",
        "with open('../../Data/Preprocessed Data/Synthetic/Validation/y_val.pkl', 'rb') as f:\n",
        "  y_val = pickle.load(f)\n",
        "\n",
        "y_val = dict2arr(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mask the ground truth data so that it does not contain QRS locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZOPXEVjqov",
        "outputId": "85a2e97a-859a-4b91-d602-baab59738ab0"
      },
      "outputs": [],
      "source": [
        "y_target = []\n",
        "for i in range(x_train.shape[0]):\n",
        "    y_target.append(x_train[i] * label_train[i])\n",
        "y_target = np.array(y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"x_shape:\", x_train.shape)\n",
        "print(\"y_shape:\", y_train.shape)\n",
        "print(\"y_target:\", y_target.shape)\n",
        "print(\"label_shape:\", label_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d7SiVyYv0FAa",
        "outputId": "9d38329d-fcf0-485d-f1c8-234e6140f152"
      },
      "outputs": [],
      "source": [
        "# Confirm the synthetic data is unpaired\n",
        "for i in range(5):\n",
        "  plt.plot(x_train[i])\n",
        "  plt.plot(y_train[i])\n",
        "  plt.title(\"Unpaired Synthetic Data: \" + str(i))\n",
        "  plt.xlabel(\"Samples\")\n",
        "  plt.ylabel(\"Amplitude ($\\mu$V)\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D80sGabItgkH",
        "outputId": "2756c871-920f-4e7f-ae3d-8c647f60a1ea"
      },
      "outputs": [],
      "source": [
        "# Confirm the QRS complexes are masked \n",
        "for i in range(5):\n",
        "    plt.plot(y_target[i])\n",
        "    plt.title(\"Masked Target Data: \" + str(i))\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Amplitude ($\\mu$V)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcYiM5A5GDaN",
        "outputId": "b8915938-3223-42a3-972a-72073a2eed75"
      },
      "outputs": [],
      "source": [
        "# Plot QRS complex locations over the contaminated data\n",
        "for i in range(10):\n",
        "    plt.plot(x_train[i])\n",
        "    plt.plot(label_train[i])\n",
        "    plt.title('QRS Complex Label: ' + str(i))\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Amplitude ($\\mu$V)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the TCN separation module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U8LT2D21e6x"
      },
      "outputs": [],
      "source": [
        "def tcn_block(inputs, filters, dilation_rates):\n",
        "  res = LayerNormalization()(inputs)\n",
        "  for dilation in dilation_rates:\n",
        "    x = SeparableConv1D(filters, dilation_rate=dilation,\n",
        "                        kernel_size=3, padding='same', activation=gelu)(res)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = SeparableConv1D(filters, dilation_rate=dilation,\n",
        "                        kernel_size=3, padding='same', activation=gelu)(x)\n",
        "    res = LayerNormalization()(Add()([x, res]))\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWcDiO59KJr9"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  dilation_rates = [1,2,4,8,16,32,64]\n",
        "\n",
        "  inputs = Input(shape=(None, 1))\n",
        "\n",
        "  x = Conv1D(filters=32, kernel_size=7, padding='same', activation=gelu)(inputs)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=128, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=128, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "\n",
        "  sep = tcn_block(x, filters=128, dilation_rates=dilation_rates)\n",
        "  sep = Conv1D(filters=128, kernel_size=1, padding='same', activation='sigmoid')(sep)\n",
        "\n",
        "  mask = Multiply()([x, sep])\n",
        "  mask = LayerNormalization()(mask)\n",
        "\n",
        "  x = Conv1DTranspose(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(mask)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1DTranspose(filters=32, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  outputs = Conv1D(1, kernel_size=7, padding='same')(x)\n",
        "  return Model(inputs, outputs, name='generator')\n",
        "\n",
        "model = build_generator()\n",
        "model.summary()\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itoO4OitvuDn"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "  dilation_rates = [1,2,4,8,16,32,64,128]\n",
        "\n",
        "  inputs = Input(shape=(None, 1))\n",
        "\n",
        "  noise = GaussianNoise(0.1)(inputs)\n",
        "\n",
        "  x = Conv1D(filters=32, kernel_size=7, padding='same', activation=gelu)(noise)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, strides=2, kernel_size=3, padding='same', activation=gelu)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  x = Conv1D(filters=64, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "\n",
        "  x = tcn_block(x, filters=64, dilation_rates=dilation_rates)\n",
        "\n",
        "  x = Conv1D(filters=64, kernel_size=1, padding='same', activation=gelu)(x)\n",
        "  x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "  outputs = Dense(1)(x)\n",
        "\n",
        "  return Model(inputs, outputs, name='discriminator')\n",
        "\n",
        "model = build_discriminator()\n",
        "model.summary()\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to create & compile the generator & discriminator neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzxTAtsZOYZJ"
      },
      "outputs": [],
      "source": [
        "def create_gans():\n",
        "\n",
        "    # create building blocks\n",
        "    discriminator = build_discriminator()\n",
        "    generator = build_generator()\n",
        "\n",
        "    # compile discriminators while they're set to trainable\n",
        "    optimizer = Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "    discriminator.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # create the first GAN architecture\n",
        "    input_seq = Input(shape=(None,1))\n",
        "    input_weights = Input(shape=(None,1))\n",
        "\n",
        "    output_seq_b = generator(input_seq)\n",
        "    global_out = discriminator(output_seq_b)\n",
        "    mask_output_seq_b = Multiply(name='reconstruction')([output_seq_b, input_weights])\n",
        "\n",
        "    GAN = Model([input_seq, input_weights], [global_out, mask_output_seq_b], name='GAN')\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-4, beta_1=0.5)\n",
        "    loss = ['mse', 'mae']\n",
        "    loss_weights = [1, 10]\n",
        "\n",
        "    GAN.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights)\n",
        "\n",
        "    return GAN, generator, discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ370E0jG9i0"
      },
      "outputs": [],
      "source": [
        "GAN, generator, discriminator = create_gans()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define evaluation metric functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SNR(y_true, y_pred):\n",
        "    diff = y_true - y_pred\n",
        "    num = np.var(y_true)\n",
        "    den = np.var(diff)\n",
        "    snr = 10*np.log10(num/den)\n",
        "    return snr\n",
        "\n",
        "def freq_transform(data, Fs):\n",
        "    # Window length of 1 second \n",
        "    nperseg = Fs*1\n",
        "    # 75% overlap\n",
        "    noverlap = np.ceil(nperseg * 0.75) \n",
        "    # Hann window\n",
        "    window = 'hann'  \n",
        "    # Zero-padding to the next power of two\n",
        "    nfft = 2**np.ceil(np.log2(nperseg))\n",
        "\n",
        "    freq, pxx = signal.welch(data, fs=Fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft)\n",
        "    return pxx\n",
        "\n",
        "def get_jsd(y_true, y_pred, Fs):\n",
        "    jsd_all = []\n",
        "    for i in range(y_pred.shape[0]):\n",
        "        y_true_pxx = freq_transform(y_true[i].squeeze(), Fs=Fs)\n",
        "        y_pred_pxx = freq_transform(y_pred[i].squeeze(), Fs=Fs)\n",
        "\n",
        "        jsd_all.append(spatial.distance.jensenshannon(y_true_pxx, y_pred_pxx, base=2))\n",
        "    return np.mean(np.array(jsd_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYqU6XDiWGek"
      },
      "outputs": [],
      "source": [
        "# Reset states generated by Keras\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_DVNmix4G9i0",
        "outputId": "37369cb7-f791-4fa7-ddca-947e267a45c1"
      },
      "outputs": [],
      "source": [
        "# Core training loop, loads batch of sequences, generators and discriminators on batch\n",
        "batch_size = 64\n",
        "\n",
        "# Calculate number of batches\n",
        "num_batches = x_train.shape[0] // batch_size\n",
        "\n",
        "# total epochs \n",
        "epochs = 150\n",
        "\n",
        "epoch_count = 1 # index of current epoch\n",
        "\n",
        "# Empty arrays for storing loss\n",
        "real_loss_all = []\n",
        "fake_loss_all = []\n",
        "gan_loss_all = []\n",
        "\n",
        "while epoch_count <= epochs:\n",
        "    start_time = time.time() # timer for the epoch to complete\n",
        "\n",
        "    # shuffle training data\n",
        "    x_train, y_train, y_target, label_train = utils.shuffle(x_train, y_train, y_target, label_train)\n",
        "\n",
        "    print('EPOCH: ' + str(epoch_count))\n",
        "\n",
        "    # Get a batch of data\n",
        "    batch_index = 0  # index of the current batch\n",
        "\n",
        "    # Store per batch loss within the epoch\n",
        "    real_loss = []\n",
        "    fake_loss = []\n",
        "    gan_loss = []\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Select a subset of the data corresponding to the indices of the batch\n",
        "        seq_a_batch = x_train[batch_index:batch_index+batch_size]\n",
        "        seq_b_batch = y_train[batch_index:batch_index+batch_size]\n",
        "        masked_seq_a_batch = y_target[batch_index:batch_index+batch_size]\n",
        "        labels_batch = label_train[batch_index:batch_index+batch_size]\n",
        "\n",
        "        batch_index += batch_size\n",
        "\n",
        "        target_batch = np.ones([len(seq_a_batch),1])\n",
        "        fake_batch = np.zeros([len(seq_a_batch),1]) \n",
        "\n",
        "        # Train discriminator on real data\n",
        "        loss = discriminator.train_on_batch(seq_b_batch, target_batch)\n",
        "        real_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "\n",
        "        # Use the generator to create predictions for the discrminator\n",
        "        seq_b_batch_fake = generator.predict(seq_a_batch, verbose=0)\n",
        "\n",
        "        # Train discriminator on fake data\n",
        "        loss = discriminator.train_on_batch(seq_b_batch_fake, fake_batch)\n",
        "        fake_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "        \n",
        "        # Fit each generator\n",
        "        loss = GAN.train_on_batch([seq_a_batch, labels_batch], [target_batch, masked_seq_a_batch])\n",
        "        gan_loss.append(np.expand_dims(np.array(loss), axis=0))\n",
        "\n",
        "    gan_loss = np.mean(np.concatenate(gan_loss, axis=0), axis=0)\n",
        "    real_loss = np.mean(np.concatenate(real_loss, axis=0), axis=0)\n",
        "    fake_loss = np.mean(np.concatenate(fake_loss, axis=0), axis=0)\n",
        "\n",
        "    print(\"--- %s seconds ---\" % np.round((time.time() - start_time),2))\n",
        "    print(\"Discriminator Real Data:\", discriminator.metrics_names[0], \"=\", real_loss)\n",
        "    print(\"Discriminator Fake Data:\", discriminator.metrics_names[0], \"=\", fake_loss)\n",
        "    print(\"GAN:\", GAN.metrics_names[0], \"=\", gan_loss[0], \"-\", GAN.metrics_names[1], \"=\", gan_loss[1], \"-\", GAN.metrics_names[2], \"=\", gan_loss[2])\n",
        "    \n",
        "    gan_loss_all.append(np.expand_dims(gan_loss, axis=0))\n",
        "    fake_loss_all.append(np.expand_dims(fake_loss, axis=0))\n",
        "    real_loss_all.append(np.expand_dims(real_loss, axis=0))\n",
        "\n",
        "    # Validate performance every 10 epochs\n",
        "    if (epoch_count % 10 == 0):\n",
        "\n",
        "        # Save models\n",
        "        MODEL_PATH = '../../Models/Synthetic/discriminator_epoch' + str(epoch_count) + '.h5'\n",
        "        discriminator.save(MODEL_PATH)\n",
        "\n",
        "        MODEL_PATH = '../../Models/Synthetic/generator_epoch' + str(epoch_count) + '.h5'\n",
        "        generator.save(MODEL_PATH)\n",
        "\n",
        "        # Save current training performance\n",
        "        np.save('../../Training Performance/Synthetic/real_loss.npy', np.concatenate(real_loss_all, axis=0))\n",
        "        np.save('../../Training Performance/Synthetic/fake_loss.npy', np.concatenate(fake_loss_all, axis=0))\n",
        "        np.save('../../Training Performance/Synthetic/gan_loss.npy', np.concatenate(gan_loss_all, axis=0))\n",
        "\n",
        "        print(\"\\nEpoch \" + str(epoch_count) + \" Prediction\")\n",
        "\n",
        "        # Predict on synthetic validation data\n",
        "        y_pred = generator.predict(x_val, batch_size=32, verbose=0)\n",
        "\n",
        "        plt.plot(x_val[0])\n",
        "        plt.plot(y_pred[0])\n",
        "        plt.title(\"Synthetic A>B Prediction\")\n",
        "        plt.show()\n",
        "\n",
        "        snr = SNR(y_val, y_pred)\n",
        "        print(\"Predicted Signal-Noise Ratio:\", snr, \"\\n\")\n",
        "\n",
        "        average_jsd = get_jsd(y_val, y_pred, Fs=1920)\n",
        "        print(\"Average Jensen-Shannon Divergence:\", average_jsd, \"Bits\")\n",
        "\n",
        "    epoch_count += 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed17bba15b2f81cf1230037794da0c0f111480f5f2f8146275a9e25508496a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
